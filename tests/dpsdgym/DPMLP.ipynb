{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchdp import PrivacyEngine, utils, autograd_grad_sample\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_layer_size, in_size, classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_size=in_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.classes = classes\n",
    "        self.forward_1 = torch.nn.Linear(self.in_size,self.hidden_layer_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.forward_2 = torch.nn.Linear(self.hidden_layer_size, self.classes)\n",
    "        self.soft_max = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        hidden_layer = self.forward_1(x)\n",
    "        # print(hidden_layer)\n",
    "        relu_step = self.relu(hidden_layer)\n",
    "        out = self.forward_2(relu_step)\n",
    "        soft_max = self.soft_max(out)\n",
    "        return soft_max\n",
    "    \n",
    "### Load dataset and split\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "loaded_datasets = load_data()\n",
    "data = loaded_datasets['car'][\"data\"]\n",
    "X = data.loc[:, data.columns != loaded_datasets['car'][\"target\"]]\n",
    "y = data.loc[:, data.columns == loaded_datasets['car'][\"target\"]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = MLP(100, X.shape[1], len(np.unique(y)))\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "        \n",
    "x_train = Variable(torch.from_numpy(x_train.to_numpy()).float(), requires_grad=True)\n",
    "y_train = Variable(torch.from_numpy(y_train.to_numpy().ravel()).float(), requires_grad=True)\n",
    "# Check test loss\n",
    "model.eval()\n",
    "x_test = Variable(torch.from_numpy(x_test.to_numpy())).float()\n",
    "y_test = Variable(torch.from_numpy(y_test.to_numpy().ravel())).float()\n",
    "y_pred = model(x_test)\n",
    "print(torch.argmax(y_pred.squeeze(),1).size())\n",
    "print(y_test.size())\n",
    "before_train = criterion(torch.argmax(y_pred.squeeze(), 1).float(), y_test)\n",
    "print('Test loss before training' , before_train.item())\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    converted = torch.argmax(y_pred.squeeze(),1).float()\n",
    "    loss = criterion(converted, y_train)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "print(torch.argmax(y_pred.squeeze(),1).size())\n",
    "print(y_test.size())\n",
    "before_train = criterion(torch.argmax(y_pred.squeeze(), 1).float(), y_test)\n",
    "print('Test loss after training' , before_train.item())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# print(torch.argmax(y_pred.squeeze(), 1).numpy())\n",
    "# print(y_test.numpy())\n",
    "accuracy_score(torch.argmax(y_pred.squeeze(), 1).numpy(), y_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchdp import PrivacyEngine, utils, autograd_grad_sample\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_layer_size, in_size, classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_size=in_size\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.classes = classes\n",
    "        self.forward_1 = torch.nn.Linear(self.in_size,self.hidden_layer_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.forward_2 = torch.nn.Linear(self.hidden_layer_size, self.classes)\n",
    "        self.soft_max = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        hidden_layer = self.forward_1(x)\n",
    "        # print(hidden_layer)\n",
    "        relu_step = self.relu(hidden_layer)\n",
    "        out = self.forward_2(relu_step)\n",
    "        soft_max = self.soft_max(out)\n",
    "        return torch.argmax(soft_max, 1).float()\n",
    "    \n",
    "### Load dataset and split\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "loaded_datasets = load_data()\n",
    "data = loaded_datasets['car'][\"data\"]\n",
    "X = data.loc[:, data.columns != loaded_datasets['car'][\"target\"]]\n",
    "y = data.loc[:, data.columns == loaded_datasets['car'][\"target\"]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = MLP(100, X.shape[1], len(np.unique(y)))\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "        \n",
    "x_train = Variable(torch.from_numpy(x_train.to_numpy()).float(), requires_grad=True)\n",
    "y_train = Variable(torch.from_numpy(y_train.to_numpy().ravel()).float(), requires_grad=True)\n",
    "# Check test loss\n",
    "model.eval()\n",
    "x_test = Variable(torch.from_numpy(x_test.to_numpy())).float()\n",
    "y_test = Variable(torch.from_numpy(y_test.to_numpy().ravel())).float()\n",
    "y_pred = model(x_test)\n",
    "print(y_pred.size())\n",
    "print(y_test.size())\n",
    "before_train = criterion(y_pred, y_test)\n",
    "print('Test loss before training' , before_train.item())\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "print(y_pred.size())\n",
    "print(y_test.size())\n",
    "before_train = criterion(y_pred, y_test)\n",
    "print('Test loss after training' , before_train.item())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# print(torch.argmax(y_pred.squeeze(), 1).numpy())\n",
    "# print(y_test.numpy())\n",
    "accuracy_score(y_pred.numpy(), y_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory consumed by mushroom:1494944\n",
      "Memory use too high with mushroom, subsampling to:1000000\n",
      "Memory consumed by mushroom:1043328\n",
      "0.00023004370830457787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lurosenb/anaconda3/envs/oss_dp_2/lib/python3.8/site-packages/torchdp/privacy_engine.py:141: UserWarning: PrivacyEngine expected a batch of size 250 but the last step received a batch of size 1. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.6696429252624512\n",
      "epsilon is 0.39726294588489375, alpha is 37.0\n",
      "Epoch 3 - loss: 0.6951021552085876\n",
      "epsilon is 0.7363773297646037, alpha is 22.0\n",
      "Epoch 6 - loss: 0.6997715830802917\n",
      "epsilon is 0.9601826185669726, alpha is 18.0\n",
      "Epoch 9 - loss: 0.5692386627197266\n",
      "epsilon is 1.1418089324100684, alpha is 15.0\n",
      "Epoch 12 - loss: 0.5020430088043213\n",
      "epsilon is 1.2987387155396775, alpha is 14.0\n",
      "Epoch 15 - loss: 0.4601525664329529\n",
      "epsilon is 1.4402855435558197, alpha is 13.0\n",
      "Epoch 18 - loss: 0.5488324761390686\n",
      "epsilon is 1.5690691188167256, alpha is 12.0\n",
      "Epoch 21 - loss: 0.931053876876831\n",
      "epsilon is 1.6886356656211716, alpha is 10.9\n",
      "Epoch 24 - loss: 0.5452947616577148\n",
      "epsilon is 1.801292100034472, alpha is 10.4\n",
      "Epoch 27 - loss: 0.48831701278686523\n",
      "epsilon is 1.9080624606533234, alpha is 9.9\n",
      "Epoch 30 - loss: 0.4481630325317383\n",
      "epsilon is 2.0097439373377393, alpha is 9.5\n",
      "Epoch 33 - loss: 0.3437044322490692\n",
      "epsilon is 2.107108403717419, alpha is 9.2\n",
      "Epoch 36 - loss: 0.31206318736076355\n",
      "epsilon is 2.2006999509589398, alpha is 8.8\n",
      "Epoch 39 - loss: 0.4013752043247223\n",
      "epsilon is 2.290943869798198, alpha is 8.6\n",
      "Epoch 42 - loss: 0.44504573941230774\n",
      "epsilon is 2.378200428485812, alpha is 8.3\n",
      "Epoch 45 - loss: 0.41867315769195557\n",
      "epsilon is 2.462848750018207, alpha is 8.1\n",
      "Epoch 48 - loss: 0.34236663579940796\n",
      "epsilon is 2.5450994265983837, alpha is 7.9\n",
      "Epoch 51 - loss: 0.7006056308746338\n",
      "epsilon is 2.625138487040807, alpha is 7.7\n",
      "Epoch 54 - loss: 0.8784865736961365\n",
      "epsilon is 2.7031724742853687, alpha is 7.5\n",
      "Epoch 57 - loss: 0.8100544214248657\n",
      "epsilon is 2.7794317254854795, alpha is 7.3\n",
      "Epoch 60 - loss: 0.43897539377212524\n",
      "epsilon is 2.853928387760524, alpha is 7.2\n",
      "Epoch 63 - loss: 0.6372039318084717\n",
      "epsilon is 2.92694281664766, alpha is 7.0\n",
      "Epoch 66 - loss: 0.764039933681488\n",
      "epsilon is 2.998369716810679, alpha is 6.9\n",
      "Epoch 69 - loss: 0.5206937789916992\n",
      "epsilon is 3.0684995912812925, alpha is 6.8\n",
      "tensor([1, 1, 1,  ..., 1, 0, 1])\n",
      "MLP Acc:0.8767249310027599\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchdp import PrivacyEngine, utils, autograd_grad_sample\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, classes, hidden_layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_layer_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_layer_sizes[0],hidden_layer_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_layer_sizes[1],classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc3(F.leaky_relu(self.fc2(F.leaky_relu(self.fc1(x), 0.2)), 0.2))\n",
    "        return x\n",
    "\n",
    "### Load dataset and split\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "loaded_datasets = load_data()\n",
    "data = loaded_datasets['mushroom'][\"data\"]\n",
    "X = data.loc[:, data.columns != loaded_datasets['mushroom'][\"target\"]]\n",
    "y = data.loc[:, data.columns == loaded_datasets['mushroom'][\"target\"]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "scaler = MinMaxScaler()\n",
    "x_train_numpy = scaler.fit_transform(x_train)\n",
    "x_test_numpy = scaler.transform(x_test)\n",
    "x_train = pd.DataFrame(x_train_numpy, columns = x_train.columns)\n",
    "x_test = pd.DataFrame(x_test_numpy, columns = x_test.columns)\n",
    "\n",
    "net = MLP(X.shape[1], len(np.unique(y)), (50,20))\n",
    "\n",
    "sample_size=len(x_train)\n",
    "batch_size=min(250, len(x_train))\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=.02, betas=(0.5, 0.9))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    net,\n",
    "    batch_size,\n",
    "    sample_size,\n",
    "    alphas= [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
    "    noise_multiplier=3.0,\n",
    "    max_grad_norm=1.0,\n",
    "    clip_per_layer=True\n",
    ")\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "target_delta = 1/x_train.shape[0]\n",
    "print(target_delta)\n",
    "for epoch in range(500):\n",
    "    for i in range(int(len(x_train)/batch_size) + 1):\n",
    "        data2 = x_train.iloc[i*batch_size:i*batch_size+batch_size, :]\n",
    "        labels = y_train.iloc[i*batch_size:i*batch_size+batch_size, :]\n",
    "        if len(labels) < batch_size:\n",
    "            break\n",
    "        X, Y = Variable(torch.FloatTensor([data2.to_numpy()]), requires_grad=True), Variable(torch.FloatTensor([labels.to_numpy()]), requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net(X)\n",
    "        output = criterion(y_pred.squeeze(), Y.squeeze().long())\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch % 3 == 0.0):\n",
    "        print(\"Epoch {} - loss: {}\".format(epoch, output))\n",
    "        epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(target_delta)\n",
    "        print ('epsilon is {e}, alpha is {a}'.format(e=epsilon, a = best_alpha))\n",
    "        if 3.0 < epsilon:\n",
    "            break\n",
    "        \n",
    "predictions = torch.argmax(net(Variable(torch.FloatTensor([x_test.to_numpy()]), requires_grad=True))[0],1)\n",
    "print(predictions)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('MLP Acc:' + str(accuracy_score(predictions.numpy(), y_test.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPLR Acc:0.9006439742410304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lurosenb/anaconda3/envs/oss_dp_2/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/lurosenb/anaconda3/envs/oss_dp_2/lib/python3.8/site-packages/diffprivlib/models/logistic_regression.py:222: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n"
     ]
    }
   ],
   "source": [
    "from diffprivlib.models import LogisticRegression\n",
    "clf = LogisticRegression(epsilon=3.0).fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "print('DPLR Acc:' + str(accuracy_score(predictions, y_test.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
